
\documentclass[a4paper]{article}
%Included packages ----------------------------------------------------------%
\usepackage[utf8]{inputenc}                        % utf-8 encoding, æ, ø , å, etc.
\usepackage{a4wide}                          % Adjust margins to better fit A4 format.
\usepackage{array}                           % Matrices.
\usepackage{amsmath}                         % Math symbols, and enhanced matrices.o
\usepackage{amsfonts}                        % Math fonts.
\usepackage{amssymb}                         % Additional symbols.
%\usepackage{wasysym}                        % More additional symbols.
\usepackage{mathrsfs}                        % Most additional symbols.
\usepackage[pdftex]{graphicx}                % Improved inclusion of .pdf-graphics files.
\usepackage{sidecap}                         % Floats with captions to the right/left.
\usepackage{cancel}                          % Visualize cancellations in equations.
\usepackage{enumerate}                       % Change counters (arabic, roman, etc.).
\usepackage{units}                           % Adds better looking fractions (nicefrac).
\usepackage{floatrow}                        % Multi-figure floats.
\usepackage{subfig}                          % Multi-figure floats.
\usepackage{caption}                         % Adds functionality to captions.
\usepackage{bm}                              % Bolded text in math mode.
\usepackage{combinedgraphics}                % Figures; let latex handle the text itself.
\usepackage[framemethod=default]{mdframed}   % Make boxes.
\usepackage{listings}                        % For including source code.
\usepackage[colorlinks]{hyperref}            % Interactive references, colored.
\usepackage{soul}                            % Make vertical bars through text.
\usepackage{nicefrac}                        % Nice fractions with \nicefrac.
\usepackage{mathtools}                       % Underbrackets, overbrackets.
\usepackage{wasysym}                         % \smiley{}-s!
\usepackage{multicol}                        % Multiple text columns.
\usepackage{capt-of}                         % Caption things which are not floats.
%\usepackage[url=false]{biblatex}             % Citations (made easy).
\usepackage{dsfont}
\usepackage{booktabs}                        % Tables
\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}% http://ctan.org/pkg/multirow
\usepackage{hhline}% http://ctan.org/pkg/hhline
\usepackage{siunitx}
\usepackage[version=4]{mhchem}
\usepackage{relsize}  % Resize parts of equations
\usepackage[backend=biber,url=false]{biblatex}
\usepackage{mdframed}
\usepackage{tikz}
\usetikzlibrary{matrix}




% Differentials -------------------------------------------------------------- %
\newcommand{\dt}{\,\mathrm{d}t}
\newcommand{\dx}{\,\mathrm{d}x}
\newcommand{\dr}{\,\mathrm{d}r}

% Derivatives ---------------------------------------------------------------- %
\newcommand{\der} [2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}   % Derivative.
\newcommand{\pder}[2]{\frac{\partial   #1}{\partial   #2}}   % Partial derivative.

% Matrices ------------------------------------------------------------------- %
\newcommand{\mat} [2]{\begin{matrix}[#1]  #2 \end{matrix}}   % Nothing enclosing it.
\newcommand{\pmat}[2]{\begin{pmatrix}[#1] #2 \end{pmatrix}}  % Enclosing parentheses.
\newcommand{\bmat}[2]{\begin{bmatrix}[#1] #2 \end{bmatrix}}  % Enclosing square brackets.
\newcommand{\vmat}[2]{\begin{vmatrix}[#1] #2 \end{vmatrix}}  % Enclosing vertical bars.
\newcommand{\Vmat}[2]{\begin{Vmatrix}[#1] #2 \end{Vmatrix}}  % Enclosing double bars.

% Number sets ---------------------------------------------------------------- %
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

% Manually set alignment of rows / columns in matrices (mat, pmat, etc.) ----- %
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

% References ----------------------------------------------------------------- %
\newcommand{\Fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\fig}[1]{Fig.\ \ref{fig:#1}}
\newcommand{\eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\Eq} [1]{Eq.\ (\ref{eq:#1})}
\newcommand{\tab}[1]{Table \ref{tab:#1}}
\newcommand{\Tab}[1]{Table \ref{tab:#1}}

% Paragraph formatting ------------------------------------------------------- %
\setlength{\parindent}{5.5mm}
\setlength{\parskip}  {0mm}

% Source code listings ------------------------------------------------------- %
\definecolor{commentGreen}{RGB}{34,139,34}
\definecolor{keywordBlue}{RGB}{0,0,255}
\definecolor{stringPurple}{RGB}{160,32,240}
\lstset{language=matlab}
\lstset{basicstyle=\ttfamily\small}
\lstset{frame=single}
\lstset{stringstyle=\color{stringPurple}}
\lstset{keywordstyle=\color{keywordBlue}}
\lstset{commentstyle=\color{commentGreen}}
\lstset{morecomment=[l][\color{commentGreen}\bfseries]{\%\%}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=true}
\lstset{columns=fixed}
\lstset{breaklines}
\lstset{literate={~} {$\sim$}{1}}
\lstset{numbers=left}              
\lstset{stepnumber=1}
\renewcommand{\ttdefault}{pcr}
\lstdefinestyle{prt}{frame=none,basicstyle=\ttfamily\small}

% Convenient shorthand notation ---------------------------------------------- %
\newcommand{\nn}{\nonumber}
\newcommand{\e}[1]{\cdot10^{#1}}
\renewcommand{\i}{\hat{\imath}}
\renewcommand{\j}{\hat{\jmath}}
\renewcommand{\k}{\hat{k}}

% Caption position of tables at the top -------------------------------------- %
\floatsetup[table]{capposition=top}

% Black frame with white background ------------------------------------------ %
\newmdenv[linecolor=black,backgroundcolor=white]{exframe}

% Including vector drawings from inkscape ------------------------------------ %
\newenvironment{combFig}[5]{
  \begin{figure}[#1] 
    \centering 
    \includecombinedgraphics[vecscale=#2, keepaspectratio]{#3} 
    \caption{#4 \label{#5}}
  \end{figure}
  }

  {
}

% Including pdf graphics ----------------------------------------------------- %
\newenvironment{pdfFig}[5]{
  \begin{figure}[#1] 
    \centering 
    \includegraphics[width= #2]{#3} 
    \caption{#4 \label{#5}}
  \end{figure}
  }

  {
}

% Exercise and subexercise counters ------------------------------------------ %
\newcounter{excounter}
\renewcommand\theexcounter{\arabic{excounter}}
\newcommand\exlabel{\theexcounter}
\setcounter{excounter}{1}

\newcounter{subexcounter}
\renewcommand\thesubexcounter{\arabic{subexcounter}}
\newcommand\subexlabel{\thesubexcounter}
\setcounter{subexcounter}{1}

% Environments for exercises ------------------------------------------------- %
\newenvironment{exercise}[1]{
  \subsection*{Exercise \theexcounter: #1}
  \setcounter{subexcounter}{1}                      % Reset the subexercise counter to a.
  \addcontentsline{toc}{section}{\theexcounter: #1} % Add the exercise to TOC
  }
      % Exercise text.
  {
  \stepcounter{excounter}                           % Add one to the exercise counter.
  \newpage
}

% Environment for subexercises ----------------------------------------------- %
\newenvironment{subexercise}{
  \begin{exframe}
    \begin{itemize}  \setlength{\itemindent}{1cm}
      \item[{\bf Exercise \thesubexcounter}] 
	}
	  % Subexercise text.
	{
    \end{itemize}
  \end{exframe}
  \stepcounter{subexcounter}                        % Add one to the exercise counter.
}

% Environment for proofs ----------------------------------------------------- %
\newenvironment{proof}[2]{
  \begin{exframe}
    \begin{itemize}  \setlength{\itemindent}{0.6cm}
      \item[{\bf #1} {\bf #2}] 
	}
	  % Subexercise text.
	{
    \end{itemize}
  \end{exframe}
}

% Environment for answers ---------------------------------------------------- %
\newenvironment{answer}{}{}

% Set bibliography file and path for images.
%\bibliography{references/fys4180ref.bib}
\graphicspath{{./images/}}
\newcommand{\includepdfgraphics}[2]{\includecombinedgraphics[#1]{./images/#2}}


\graphicspath{{/Users/morten/Documents/Master/Master/Figures/}}


% Title
\title{}
\date{}
\author{}
% ---------------------------------------------------------------------------- %
% ---------------------------------------------------------------------------- %

\newcommand{\comment}[1]{\ignorespaces}
\addbibresource{/Users/morten/Documents/Master/Master/ref.bib}
%\bibliography{/Users/morten/Documents/Master/Master/ref.bib}%{}
%\bibliographystyle{plain}
\begin{document}


\renewcommand{\R}{{\bf R}}
\renewcommand{\r}{{\bf r}}
\newcommand{\p}{{\bf p}}
\newcommand{\q}{{\bf q}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\psit}{\left|\psi(t)\right\rangle}



%Atomic Reference Data for Electronic Structure Calculations \url{http://math.nist.gov/DFTdata/atomdata/node4.html}


\section{Density functional theory}
%\subsection{Intro and motivation}
Because the wave function is such an unbelievably complicated function, depending on $4N$ degrees of freedom (of which $3N$ are spatial coordinates), it is natural to ask the question: Is it possible to represent the state of an electronic system in a more succinct way? A natural candidate for such an entity is the electronic number density, $\rho(\r)$, which we will mostly refer to as simply \emph{the density}. It would be remarkable if we could deal with enormously complex quantum mechanical systems by means of a function depending only on three spatial coordinates and spin(!) \cite{kryachko} \comment{p163}.. 

It turns out that exactly this is possible. There is a one-to-one correspondance between the ground state density, $\rho_0(\r)$, and the external potential (up to an additive constant) and thus also the Hamiltonian \cite{toulouse}\comment{p5}. Since the Hamiltonian uniquely determines all properties of a quantum mechanical system, we can in principle determine all the information in the many-body wave function (of the ground state and \emph{all} excited states) from the ground state density alone \cite{martin}\comment{p119}. The fact that the density can be determined from the wave function is almost trivially true, but that the converse is true is the content of the Hohenberg-Kohn theorems. 

However, the theorems of Hohenberg and Kohn guarantee only the existence and uniqueness of an \emph{energy functional}, $E[\rho]$, which can be used to determine the energy from the density. Without knowing the form of $E[\rho]$ and a computational scheme for calculating it, we are still no closer to being able to use the electron density as the basic variable in electronic structure calculations. This is where the Kohn-Sham ansatz comes in, making it possible for us to calculate structure properties of electronic systems by essentially solving a different system\----a non-interacting system with the electrons moving in an effective potential which by construction yields the same ground state density as the original system. 

We will begin our discussion of {\bf Density functional theory} (DFT [sometimes prepended KS, making it Kohn-Sham density functional theory \{KS-DFT\}]) by considering the theoretical framework and the Hohenberg-Kohn theorems. Then we will consider the Kohn-Sham ansatz and how DFT calculations are performed in practice.


\subsection{The Hohenberg-Kohn theorems}
Recall from ((Born-oppenheimer section)) that the \emph{electronic} Hamiltonian under the Born-Oppenheimer approximation takes the form
\begin{align}
\hat H &= -\frac{1}{2}\sum_{i=1}^N\nabla^2_i + \sum_{i=1}^N\sum_{j=i+1}^N \frac{1}{|\r_i-\r_j|} - \sum_{i=1}^N\sum_{A=1}^M \frac{Z_A}{|\r_i-\r_A|}.
\end{align}
We will in the following relabel the last term, and define
\begin{align}
\sum_{i=1}^{N}\sum_{A=1}^M \frac{-Z_A}{|\r_i-\r_A|} &\equiv \hat V_\text{ext}.
\end{align}
Since the first two terms of the Hamiltonian are the same for any system of $N$ electrons, the external potential term $\hat V_\text{ext}$ completely fixes the electronic Hamiltonian as a whole. In fixing $\hat H$, we also fix the spectrum and so the ground state and all its derived properties are determined by $N$ and $\hat V_\text{ext}$ alone \cite{yangparr}\comment{p51}. One such property is of course the ground state electronic density.

In the following, we will denote by $v(\r)$ the spatial (position basis) representation of the external potential $\hat V_\text{ext}$

\subsubsection*{The first Hohenberg-Kohn theorem}
\begin{figure}
\begin{center}
\begin{tikzpicture}
  \matrix (m)[
    matrix of math nodes,
    nodes in empty cells,
    minimum width=width("998888"),
  ] {
  \hat V_\text{ext}(\r) &  \mathlarger{\mathlarger{\xleftarrow{\text{Hohenberg-Kohn}}}} & \rho_0(\r) \\
& & \\
\mathlarger{\mathlarger{\downarrow}} & & \mathlarger{\mathlarger{\uparrow}} \\
& & \\
\hat H & \mathlarger{\mathlarger{\xrightarrow{\text{Schrödinger equation}}}} & \Psi(\R) \\
};
  \draw (-4,-2) -- (4,-2);
  \draw (-4,-2) -- (-4,2);
  \draw (4,-2) -- (4,2);
  \draw (-4,2) -- (4,2);
\end{tikzpicture}
\caption{Schematic representation of the Hohenberg-Kohn theorems. Knowing the external potential fixes the Hamiltonian, from which we can extract the spectrum. The ground state density may then be extracted from the ground state wave function. The Hohenberg-Kohn theorems allows us (in principle) to find the potential if we know only the ground state wave function, making a one-to-one correspondence, $\hat V_\text{ext}(\r)\Leftrightarrow\rho_0(\r)$.  Adapted from a similar figure in \cite{martin}\comment{p112}.  \label{fig:DFT1}}
\end{center}
\end{figure}

As mentioned already, the external potential trivially determines the electron density. The first theorem of Hohenberg and Kohn proves the highly non-trivial converse statement\----the external potential is uniquely determined (up to an additive constant) by the ground state electron density \cite{hohenberg-kohn}. We outline the deceptively simple proof in the following paragraphs.

Consider two potentials, $\hat V_1$ and $\hat V_2$, differing by more than an additive constant, $\hat V_1 \not= \hat V_2 + \text{const}$ and denote the ground state energies of the corresponding Hamiltonians by $E_1$ and $E_2$ respectively. Assume now that the ground state wave functions of the two Hamiltonians are \emph{the same}, $\Psi_1(\R)=\Psi_2(\R)$. Since all parts of the Hamiltonian apart from $\hat V_\text{ext}$ coincide, subtracting the two Schrödinger equations give
\begin{align}
\hat H_1|\Psi\rangle - \hat H_2 |\Psi\rangle = \left(\hat V_1 - \hat V_2\right)|\Psi\rangle &= \left(E_1-E_2\right)|\Psi\rangle.
\end{align}
In terms of the wave functions (projecting the equation onto the position basis) this yields
\begin{align}
\sum_{i=1}^N \big[\hat v_1(\r_i) - \hat v_2(\r_i)\big] \Psi(\r_1,\r_2,\dots,\r_N) &= \left(E_1-E_2\right) \Psi(\r_1,\r_2,\dots,\r_N),
\end{align}
which means $\hat V_1-\hat V_2=\text{const}$, in contradiction with the assumption. 

We proceed thus with $\Psi_1(\R)$ and $\Psi_2(\R)$ neccessarily different. Assume now that $\Psi_1(\R)$ and $\Psi_2(\R)$ have the same ground state electronic density, $\rho_0(\r)$. From the variational principle ((Ref var principle section)), we know that 
\begin{align}
E_1=\big\langle \Psi_1 |\hat H_1 |\Psi_1\big\rangle &< \big\langle \Psi_2 |\hat H_1 |\Psi_2\big\rangle \nn\\
%
&= \big\langle \Psi_2 | \hat H_2 + \hat V_1 - \hat V_2 |\Psi_2\big\rangle \nn\\
%
&= E_2 + \int \,\mathrm{d}^3\r \big[v_1(\r) - v_2(\r) \big] |\Psi_2(\R)|^2 \nn\\
%
\Rightarrow \ \ E_1 &< E_2 + \int \,\mathrm{d}^3\r \big[v_1(\r) - v_2(\r) \big] \rho_0(\r). \label{eq:dft1}
\end{align}
Exchanging the arbitrary indices $1\leftrightarrow2$, gives rise to 
\begin{align}
E_2 &< E_1 + \int \,\mathrm{d}^3\r \big[v_2(\r) - v_1(\r) \big] \rho_0(\r), \label{eq:dft2}
\end{align}
and adding \eq{dft1} from \eq{dft2} gives finally the contradiction $E_1+E_2 < E_1 + E_2$ since the integrals differ only by a sign. But this means that clearly, different $\hat V_\text{ext}$ neccessarily produce differing $\rho_0(\r)$ \cite{toulouse}\comment{p5}.\footnote{As a hands-on example of the first theorem in practice, it is instructive to consider the Coulombic external potential of $M$ stationary nuclei, $\hat V=-\sum_{A=1}^M Z_A/|\r-\r_A|$. In order to uniquely determine the system, we need to know the number of electrons, $N$, the position of the nuclei, $\r_A$, and their charges, $Z_A$. The local maxima of the ground state density coincides perfectly with the nuclei positions. Furthermore, the Kato cusp condition states that at the nuclei ((ref cusp condition section)) $(\partial \bar\rho_0(r_A)/\partial r_A)_{r_A\rightarrow 0}=-2Z_A\bar\rho_0(0)$, where $\bar\rho(r_A)$ denotes the spherical average of density around nucleus $A$ and $r_A=|\r-\r_A|$. This determines $Z_A$ from the ground state density also. Finally, the integral over the density itself gives the number of electrons, $N=\int\,\mathrm{d}^3\r \rho_0(\r)$. 

This is known as E. Bright Wilson's observation \cite{roos}\comment{p92}.}

The statement of the first Hohenberg-Kohn theorem is represented in diagram form in \fig{DFT1}.

\subsubsection*{The second Hohenberg-Kohn theorem}
Since, by the first theorem, the Hamiltonian is determined uniquely by the density, that means the wave function can be considered a functional of the density also, $\Psi[\rho]$. Following the original article by Hohenberg and Kohn\cite{hohenberg-kohn}, we note that this means the kinetic energy operator and the electron-electron interaction operators are also both functionals of the density. These can be combined into the \emph{universal functional}, 
\begin{align}
F[\rho(\r)] \equiv \big\langle \Psi|\hat T + \hat W|\Psi \big\rangle.
\end{align}
A further energy functional, $E_V[\rho]$, can be defined as 
\begin{align}
E_V[\rho(\r)] &\equiv F[\rho(\r)] + \int\mathrm{d}^3\r \,v(\r)\rho(\r),
\end{align}
where $v(\r)$ is the position basis representation of an arbitrary external potential. It is clear that minimizing $E_V[\rho]$ will yield the ground state energy of the system with Hamiltonian $\hat H = \hat T + \hat W + \hat V_\text{ext}$. In the present section we restrict ourselves to densities representing a system of $N$ electrons, i.e. fixing $\int\mathrm{d}^3\r\,\rho(\r)=N$.

We now consider the energy functional evaluated at some other density, $\rho(\r)\not=\rho_0(\r)$, with $\rho_0(\r)$ being the corresponding density of the ground state of $\hat H$, $\rho_0(\r)=\int|\Psi_0(\R)|^2$. The other density, $\rho(\r)$, is taken to be the corresponding ground state density of \emph{some other} external potential, $\hat V_\text{ext}'$. Evaluating the functional gives
\begin{align}
E_V[\rho(\r)] &= \big\langle\Psi|\hat T + \hat W|\Psi\big\rangle + \big\langle\Psi|\hat V_\text{ext}|\Psi\big\rangle = \big\langle \Psi|\hat H|\Psi\big\rangle.
\end{align}
Having established that the external potential is a functional of the density, this gives us now almost trivially that $E_V[\rho]>E_V[\rho_0]$ for any density that is not the one associated with the true ground state corresponding to $\hat V_\text{ext}$ \cite{martin}\comment{p125}. 

\subsubsection*{The way forward}
Essentially, the two Hohenberg-Kohn theorems say that all properties of a electronic quantum mechanical system is uniquely determined by the ground state density alone. Also, a \emph{universal functional} for the energy in terms of the density, valid for any external potential, exists and it attains a global minimum for exactly the true ground state density $\rho_0(\r)$. In the words of Hohenberg and Kohn: "If $F[\rho]$ were a known and sufficiently simple functional of $\rho$, the problem of determining the ground-state energy and density in a given external potential would be rather easy(...)." \cite{hohenberg-kohn} Indeed, all of electronic structure theory would have been \emph{solved} in one fel swoop. 

Predictably, this is not the case. Determination of the universal functional is anything but trivial. In order to make real progress with the approach of considering the electronic density as the primary variable, we turn now to the framework proposed by Kohn and Sham.

\subsection{Kohn-Sham ansatz}
In their seminal 1965 paper \cite{hohenberg-kohn}, Kohn and Sham outlined a way to obtain a set of single electron equations in the density that can be solved self-consitently to obtain the total electronic energy. In order to accomplish this, they consider an auxiliary non-interacting system in place of the original one \cite{martin}\comment{p135}. 
 
Kohn and Sham tell us to consider a non-interacting system for which the ground state density is the same as the ground state density for the interacting system in question. The existance of such a non-interacting system of electrons is far for obvious. In fact, determining neccessary constraints on $\hat V_\text{ext}$ for it to be \emph{non-interacting} $v$\emph{-representable} is still an open question in the theoretical foundations of density functional theory. For almost all real world problems, it is neccessary to simply assume the existance of a non-interacting system for which the ground state density coincides with that of $\hat V_\text{ext}$ \cite{engel}\comment{p71}\cite{martin}\comment{p145}. The assumption that such a non-interacting system exists constitutes the Kohn-Sham ansatz.

We denote the non-interacting Hamiltonian and the corresponding effective potential by $\hat H_s$ and $\hat V_s$, with spatial representation $v_s(\r)$. Since the electrons in this auqiliary system dont feel the coulombic inter-electron force, we know that the \emph{exact} ground state is represented by a Slater determinant filled with the energetically lowest $N$ single electron solutions of the single electron Schrödinger equation \cite{yangparr}\comment{p143}. Note that the Schrödinger equation is by construction separable, since it only involves the kinetic energy operator and a (multiplicative) external potential operator. The one-electron Hamiltonian takes the form
\begin{align}
\hat h_s \psi_i = \left[-\frac{1}{2}\nabla^2 + v_s(\r)\right]\psi_i = \varepsilon_i \psi_i,
\end{align}
with eigenstates $\psi_i$. Since $\hat H_s$ is spin-independent\footnote{If the original Hamiltonian is spin-dependent, the effective potential needs to also carry spin-dependency in order to give the required spin-density. This is a complication we will fully ignore here.} and obviously commutes with $\hat S_z$, we may choose the $\psi_i$ to be products of spatial orbitals and the normalized spinor eigenstates of $\hat S_z$, $\psi_i(\r,\sigma)=\phi_n(\r)\chi(\sigma)$. We note that the quantum number $i$ represents both spatial and spin quantum numbers. This means we can write the exact ground state wave function and the density as 
\begin{align}
\Psi_0(\R)&=\frac{1}{\sqrt{N}}\vmat{cccc}{
\psi_1(\r_1) & \psi_2(\r_1) & \dots & \psi_N(\r_1) \\
\psi_1(\r_2) & \psi_2(\r_2) & \dots & \psi_N(\r_2) \\
\vdots & \vdots & \ddots & \vdots \\
\psi_1(\r_N) & \psi_2(\r_N) & \dots & \psi_N(\r_N)},
\end{align}
with 
\begin{align}
\rho_0(\r) &= \sum_{i=1}^N |\psi_i(\r)|^2 = \sum_\sigma \sum_{n=1}^{N/2} |\phi_n(\r)|^2 = 2\sum_{n=1}^{N/2} |\phi_n(\r)|^2.
\end{align}

In general, the kinetic energy operator as a functional of the density is not known. However, in the auqiliary non-interacting system, we can write down the closed form expression in terms of the $\psi_i$s \cite{yangparr}\comment{p143}:
\begin{align}
T_s[\rho(\r)] &= -\sum_{i=1}^N\big\langle \psi_i|-\frac{1}{2}\nabla_i^2|\psi_i\big\rangle.
\end{align}
Recall that the universal energy functional has the form $F[\rho]=T[\rho]+W[\rho]$, i.e. the sum of the kinetic and the inter-electronic potential functionals. Kohn and Sham now suggest we rewrite $F[\rho]$ in terms of the known non-interacting kinetic energy functional as
\begin{align}
F[\rho]&= T_s[\rho] + J[\rho] + E_\text{xc}[\rho],
\end{align}
where $J[\rho]$ is the classical coulomb interaction functional and $E_\text{xc}[\rho]$ is essentially \emph{everything that is left over} \cite{hohenberg-kohn}. The $J[\rho]$ functional we will shortly see is the functional form of the $\hat J$ operator in the Hartree-Fock approximation ((HF section)), while $E_\text{xc}[\rho]$ is called the exchange-correlation energy and contains the (hopefully small) correction needed for $T_s[\rho]$ in order to make it into $T[\rho]$ and the non-classical part of the electron-electron interaction $\hat W$ \cite{yangparr}\comment{p144}\cite{martin}\comment{p138}. 

The \emph{exact} (recall that the density of the auqiliary system is the same as the original density) energy functional of the fully interacting system can now be written as 
\begin{align}
E[\rho] &= T_s[\rho] + J[\rho] + \int\mathrm{d}^3\r\,\rho(\r)v(\r) + E_\text{xc}[\rho],
\end{align}
where $v(\r)$ is the position basis representation of $\hat V_\text{ext}$. For completeness, we state also the explicit form of the exchange-correlation energy 
\begin{align}
E_\text{xc}[\rho] &= \left(T[\rho] - T_s[\rho] \right) + \left(W[\rho] - J[\rho]\right),
\end{align}
where $W[\rho]=J[\rho]-K[\rho]$ is the functional corresponding to the two-body term in the original Hamiltonian. 

It may seem like we have made no progress at all: We started off with an unknown functional $F[\rho]$, and after introducing the Kohn-Sham ansatz we are still left with an unknown (albeit different) functional, $E_\text{xc}[\rho]$. The exchange-correlation functional, however, may be easier to approximate. Also, we have introduced a separable (by construction) Hamiltonian from which we can extract a set of coupled one-electron Schrödinger equations which we may solve in order to obtain the true ground state density (and in principle all other properties of the system). Note carefully that at this point, no approximations (beyond Born-Oppenheimer and the assumption that the original density was non-interacting $v$-representible) have been made, and so we may consider the preceding results to be exact. This represents a conceptually major difference between DFT and Hartree-Fock theory: The latter is an approximate set of equations which we solve exactly, while DFT constitutes a set of exact equations which we are forced to solve approximately (because we dont know closed form expressions for $E_\text{xc}$).



\subsection{The Kohn-Sham equations}
In the same way Roothan and Hall introduced orbitals to the Hartree-Fock formalism ((ref H-F section)), we now introduce a set of spin-orbitals in the DFT scheme. The single determinantal wave function is constructed from $N$ orthonormal spin-orbitals, $\{\psi_i(\r)\}_{i=1}^N$, where we have absorbed the spin quantum number into the label $i$. In the restricted case, this means that $i$ and $i+1$ are spatially pair-wise identical, with differing spin index. The energy functional can now be written in terms of the orbitals as 
\begin{align}
E[\{\psi_i\}] = \sum_{i=1}^N 
\end{align}



\subsection{Numerical integration grids}
Since the integral of the exchange-correlation potential over the density is rarely (if ever) calculable analytically, we are forced to resort to numerical integration schemes in order to evaluate it. In the one dimensional case, strong and robust integration schemes based on gaussian quadrature integrate any sufficiently \emph{smooth} function to high accuracy using a modest number of integration points \cite{hjorthjensen}\comment{p116}. However, in multiple dimensions, the problem of numerically integrating a multi-variable function is considerably more challenging. Although the philosophy behind is unchanged, the actual application required a lot more thought. This is in large part because all the nuclei represent singularities which hinder the sucessful application of products of gaussian quadrature rules \cite{voronoi1}. Before we begin discussing integrals in density functional theory, we refer the reader to a short introduction to numerical integration in section \ref{numericalintegration} of the appendix. 

The integrals of interest within the framework of density functional theory are integrals over the electronic density. Since we know a priori that the density falls off exponentially in the long range limit (c.f. section \ref{wavefunction}), this reduces to a locally contained integral in some finite and \emph{small} region in space close to the nuclei. Since the potentials are in general functions of the density and its derivatives (which of course also falls off exponentially) we are guaranteed that the exchange-correlation potentials also vanishes exponentially as we move far away from the nuclei. 

\subsubsection{Simple spherical grid}
When dealing with a single nucleus, we may simply choose some cut-off radius and place integration points either linearly or in some other fashion along the radius up to this cut-off radius. Since we are primarily interested in integrands which fall off exponentially with the radius, having more points closer to the nucleus will yield a better approximation with fewer points. A simple example is logarithmically spaced points along the radius. At every such radius, we place $k^2$ angular points, $m$ linearly spaced points in the polar and the azimuthal angle respectively.

The weight for each of the points is simply the volume element of the sphere at radius $r_i$, polar angle $\theta_j$, and azimuthal angle $\phi_k$. In addition we need to multiply by the finite change in $r$, $\theta$, and $\phi$. In total, we recover the familiar spherical polar volume element \cite{rottmann}\comment{p69}  { }with the addition of $\Delta r \Delta \theta \Delta \phi$,
\begin{align}
w_{ijk} &= r_i^2\sin\theta_j (\underbrace{r_{i}-r_{i-1}}_{\Delta r_i}) (\underbrace{\theta_j-\theta_{j-1}}_{\Delta \theta_j})(\underbrace{\phi_k-\phi_{k-1}}_{\Delta \phi_k}) \nn\\
&= r_i^2\sin\theta_j \Delta r_i \left(\frac{\pi}{m-1}\right)\left(\frac{2\pi}{m-1}\right) \nn\\
&= \frac{2\pi r_i^2\sin\theta_j \Delta r_i}{(m-1)^2}.
\end{align}

\newcommand{\drv}{\,\mathrm{d}^3{\bf r}}
Under this approximation, the numerical integral over a functional of the density, $F[\rho]$, is given by (in terms of cartesian coordinates)
\begin{align}
\int F[\rho]\drv \approx \sum_{i=1}^n\sum_{j=1}^m\sum_{k=1}^m w_{ijk}F\left[\rho(x_\alpha,y_\beta,z_\gamma)\right],
\end{align}
with $x_\alpha=r_i\sin\theta_j\cos\phi_k$, $y_\beta=r_i\sin\theta_j\sin\phi_k$, and $z_\gamma=r_i\cos\theta_j$ being the normal transformation form spherical polar $\rightarrow$ cartesian coordinates.

The cut-off radius may be chosen to be the radius at which the most diffuse basis function has fallen to below some pre-assigned threshold value, $\varepsilon$. With cartesian gaussian basis functions, $\{\chi_b(\r)\}_{b=1}^M=\mathcal{B}$, the cut-off takes the value 
\begin{align}
r_\text{cut-off}=\sqrt{\frac{\log\varepsilon}{\min\left\{|\alpha_b|:\chi_b\in\mathcal{B} \right\}}}.
\end{align}


\subsubsection{Efficiency of angular grids and the \emph{product Gaussian quadrature formula}}
The naive grid described in the previous section \emph{works}, but is far from optimal. In order to derive a quadrature rule more suited to our purpose, we turn our attention to the general problem of integrating a function $f(\theta,\phi)$ on the surface of a unit sphere $\mathbb{S}^2=\{\r\in\mathbb{R}^3:|\r|=1\}$. Let us now ask: Is there an \emph{optimal} way to distribute points and find associated weights such that the approximation
\begin{align}
I = \int_0^\pi\mathrm{d}\theta \int_0^{2\pi}\mathrm{d}\phi \,f(\theta,\phi) \approx \sum_{i=1}^nw_if(\theta_i,\phi_i) \label{eq:sphericalapprox}
\end{align}
is the best possible approximation for a given number of points $n$? In order to make the question more precise, we note that any such function $f:\mathbb{S}^2\rightarrow\mathbb{R}$ may be expressed in terms of the spherical harmonics (since they form a complete basis for the square integrable functions on $\mathbb{S}^2$, $L^2(\mathbb{S}^2)$ \cite{atkinson}. So let us rephrase the question as: Is there a way to distribute points and find associated weights such that the approximation in \eq{sphericalapprox} integrates \emph{exactly} any linear combination of spherical harmonics with degree up to $L$?

Before we go on to answer this question, let us define more rigorously what we mean by an expansion in terms of spherical harmonics. The spherical harmonics themselves are given by \cite{rottmann}
\begin{align}
Y^m_l(\theta,\phi)=\frac{1}{\sqrt{2\pi}}P^m_l(\cos\theta)e^{im\phi},
\end{align}
with $-l\le m \le l$ and $l,m\in\mathbb{N}$. The polynomials $P^m_l$ are the \emph{normalized associated Legendre polynomials}.\footnote{The associated Legendre polynomials are solutions to the differential equation
\begin{align}
\der{}{x}\left[\left(1-x^2\right)\der{}{x}P^m_l(x)\right]+\left[ l(l+1)-\frac{m^2}{1-x^2} \right]P^m_l(x)=0,
\end{align}
which attains non-zero and non-singular solutions in $[-1,1]$ if and only if $m$ and $l$ are both integers, with $l\ge m$ \cite{rottmann}\comment{p92}. An explicit formula for the polynomials is for example 
\begin{align}
P^m_l(x)=\frac{(-1)^m}{2^ll!}\sqrt{\left(1-x^2\right)^m}\der{^{l+m}}{x^{l+m}}\left(x^2-1\right)^l.
\end{align}
} Expanding $f$ in terms of these harmonics constitutes finding $c_{lm}$,
\begin{align}
\tilde f(\theta,\phi)=\sum_{l=0}^L\sum_{m=-l}^lc_{lm}Y^m_l(\theta,\phi) \label{eq:ftilde}
\end{align}
such that $e=\lVert\tilde f - f \rVert$ is minimized \cite{matinf5620} (in the sense that the "error" $e$ is orthogonal to the finite dimensional subspace of $L^2(\mathbb{S}^2)$ spanned by the harmonics up to and including degree $L$). This is known as the Galerkin method, and the governing equation for the coefficients is the inner product 
\begin{align}
c_{lm}&=\int_{\mathbb{S}^2}\mathrm{d}\theta\mathrm{d}\phi\,f(\theta,\phi)Y^m_l(\theta,\phi).
\end{align}
The decay rate of the coefficients with increasing $l$ determines how well $\tilde f$ approximates $f$, i.e. the convergence rate of the spherical harmonic expansion \cite{beentjes}\comment{p2}.

Dealing with the problem of integrating functions across the surface of a sphere, McLaren defined in 1963 a measure of the effectiveness of a quadrature rule by how high order $L$ the rule would integrate \emph{exactly} using $K$ independent arbitrary variables used. In terms of the number of points used, the McLaren efficiency is defined as \cite{mclaren}
\begin{align}
E &= \frac{\text{Number of spherical harmonics up to and including order }L}{\text{Number of variables associated with }N\text{ points}} =  \frac{(L+1)^2}{3N}.
\end{align}
The total number of independent variables is 3 for each point\----two angles and a weight. The total number of spherical harmonics up to and including order $L$ is 
\begin{align}
\sum_{l=0}^L\sum_{m=-l}^l1 = \sum_{l=0}^L(2l+1) = \sum_{l=0}^L2l +\sum_{l=0}^L1 = L(L+1)+(L+1)=(L+1)(L+1)=(L+1)^2.
\end{align} 
In general we expect $E\le1$, although in rare cases $E>1$ \cite{atkinson}\comment{p188}.

Let us now consider the efficacy of the common practice of applying a Newton-Cotes quadrature rule (for example the trapezoidal rule) in $\phi$ and a Gauss-Legendre scheme for the $\theta$ integral. An equally spaced grid of $M$ points ensures exact integration of 
\begin{align}
\int_0^{2\pi}\mathrm{d}\phi\,e^{im\phi},
\end{align}
for all $m\le M$, whereas a gaussian quadrature rule using the Legendre polynomials gives exact integration of the associated Legendre polynomials
\begin{align}
\int_0^\pi \mathrm{d}\theta\, P_l^m(\cos\theta),
\end{align}
for $l\le M$ if $(M+1)/2$ points are used \cite{beentjes}\comment{p3}. In total, the "product Gaussian quadrature formula" takes the form \cite{atkinson}\comment{p169}
\begin{align}
I\approx \sum_{i=1}^{M}w_i\sum_{j=0}^{\frac{L+1}{2}} w_j f(\theta_i,\phi_j),
\end{align}
and the efficiency is $E=2/3$ \cite{mclaren}.

\subsubsection{Lebedev quadrature}
Since the product Gaussian quadrature fails to attain $E\approx1$, it is natural to consider it's flaws and look for an improved scheme. An obvious weakness of the product scheme is the clustering of integration points at the poles. Because the spacing in the polar angle $\theta$ is linear and there are a constant number of azimuthal angle $\phi$ points for every $\theta_i$, the distance along the surface of the unit sphere between adjacent angular points $\phi_j$ and $\phi_{j+1}$ approaches zero as $\theta\rightarrow0$ or $\pi$. Intuitively, the points close to the poles should not be more important than the points at the equator, since we are fully allowed to rotate the coordinate system $90^\circ$ along the polar angle\----essentially switching the poles and the equator\----without affecting the value of the integral.

In fact, this very observation is crucial in order to make progress: Any rotation or inversion (all points reversed through a given plane) that leaves the sphere invariant should also leave the quadrature invariant. This observation is the contents of a theorem due to Sobolev (for an english translation of his seminal 1962 paper, see e.g. \cite{sobolev}\comment{p461}). Essentially, Sobolev states that given a qudrature rule to integrate a spherical harmonic monomial\footnote{I.e. $f(\theta,\phi)=Y^m_l(\theta,\phi)$ for some single unique $l$ and $m$.} $f$ on $\mathbb{S}^2$, $I(f)$, the rule must neccessarily give the same result as the quadrature rule resulting from first applying a rotation or inversion,
\begin{align}
I(f)=I(\gamma[f]),
\end{align}
\emph{if $f$ is invariant under $\gamma$}. Here, $\gamma\in G$ and $G$ denotes the discrete symmetry group of the sphere. The theorem further states crucially that in order for $I$ to integrate all spherical harmonic monomials (and thus also all linear combinations of harmonics, such as the approximation $\tilde f$ from /eq{ftilde}) it is sufficient to only demand $I$ integrate exactly all the monomoials which are invariant under $\gamma\in G$ \cite{atkinson}\comment{p185}. 

In the 1970s, Lebedev constructed a class of invariant quadratures based on this idea by working out the invariant spherical harmonics and solving the resulting system of non-linear equations for the points and weights \cite{lebedev}\cite{beentjes}\comment{p4}.


Voronoi grid: \cite{voronoi1} \cite{voronoi2}
Becke grid: \cite{beckegrid}




\subsection{Local density approximation}
\subsubsection{VWN-LDA}
((YangAndParr, pp275)) ((atomic reference, \url{DFTdata/atomdata/node4.html})) Parametrize the exchange correlation terms: After Vosko, Wilk, and Nusair \cite{vwn}

\textbf{Exchange term}:
\begin{align}
\varepsilon_\text{x} (r_s,\xi) = \varepsilon_\text{x}^P(r_s) + \left[\varepsilon_\text{x}^P(r_s) - \varepsilon_\text{x}^F(r_s) \right] f(\xi),
\end{align}
with 
\begin{align}
r_s &\equiv \left( \frac{3}{4\pi \rho(\r))} \right)^{\nicefrac{1}{3}}, \ \ \ \leftarrow \text{ electron gas parameter} \\
%
\xi &\equiv \frac{\rho_\uparrow(\r) - \rho_\downarrow(\r)}{\rho_\uparrow(\r) + \rho_\downarrow(\r)} \ \ \ \leftarrow \text{ spin polarization}.
\end{align}

\begin{align}
\varepsilon_\text{x}^P(r_s) &= - 3 \left( \frac{9}{32\pi^2} \right)^{\nicefrac{1}{3}} \frac{1}{r_s}, \\
%
\varepsilon_\text{x}^F(r_s) &= - 3(2^{\nicefrac{1}{3}}) \left( \frac{9}{32\pi^2} \right)^{\nicefrac{1}{3}} \frac{1}{r_s},
\end{align}
and
\begin{align}
f(\xi) = \frac{(1+\xi)^{\nicefrac{4}{3}} + (1-\xi)^{\nicefrac{4}{3}} -2}{2(2^{\nicefrac{1}{3}} - 1)}.
\end{align}

In the limit of no spin-polarization (restricted), $\varepsilon_\text{x}$ takes the value
\begin{align}
\varepsilon_\text{x}^{\xi=0}(r_s)= -\frac{3}{2}\left(\frac{3 \rho(\r)}{\pi}\right)^{\nicefrac{1}{3}} \ \ \ \ \ \ \ \text{possibly missing a factor } \frac{1}{2} \text{?}
\end{align}

\textbf{Correlation term}:
\begin{align}
\varepsilon_\text{c}(r_s) &= \frac{A}{2}\left\{ \ln\left(\frac{x}{X(x)}\right) + \frac{2b}{Q}\tan^{-1}\left(\frac{Q}{2x+b}\right) \right.\nn\\
& \ \ \ \ \ \ \ \left.- \frac{bx_0}{X(x_0)} \left[\ln\left(\frac{(x-x_0)^2}{X(x)} \right) + \frac{2(b+2x_0)}{Q}\tan^{-1}\left(\frac{Q}{2x-b} \right) \right]  \right\},
\end{align}
with 
\begin{align}
x &\equiv \sqrt{r_s}, \\
%
X(x) &\equiv x^2+bx+c, \\
%
Q &\equiv \sqrt{4c-b^2}.
\end{align}

For $\xi=0$ we have $A=0.0621814$, $x_0=-0.409286$, $b=13.0720$, and $c=42.7198$.


%\printbibliography



\section{Appendix}
\subsection{Basics of numerical integration\label{numericalintegration}}
\subsubsection{Riemann integral and Riemann integrable functions}
Given a function $f(x)$ and a closed finite subset of $\mathbb{R}$, $[a,b]$ with $a<b$, a \emph{Riemann sum} of $f$ is defined as the sum of values attained on $n$ sub-intervals of $[a,b]$, i.e.
\begin{align}
S_n = \sum_{i=1}^n (x_i-x_{i-1}) \, f_i.
\end{align}
The $x_i$s here define the partitionining into sub-intervals $[x_{i-1},x_i]$ (i.e. $a=x_0<x_1<\dots< x_{n-1}<x_n=b$), while $f_i\equiv f(\xi_i)$ with $\xi_i$ \emph{some} point in sub-interval $i$. 

A sufficient condition for the \emph{Riemann integral} to exist for the function $f$ is that \emph{any} such sum (any choice of $x_i$ [for which $\max_{i}|x_i-x_{i-1}|\rightarrow0$] and $\xi_i$) converge to the same value in the limit $n\rightarrow \infty$ \cite{davis}\comment{p7}. In this case we say
\begin{align}
\lim_{n\rightarrow \infty} S_n = S = \int_a^b f(x)\dx,
\end{align} 
and that $f$ is Riemann integrable.

A less strict, but still sufficient condition is to chose $\overline{f_i}=\max\{f(x):x\in[x_{i-1},x_i]\}$ and $\underline{f_i}=\min\{f(x):x\in[x_{i-1},x_i]\}$ and then only demand that the two sums converge to a common limit \cite{lindstrom}\comment{p366}, 
\begin{align}
\mat{rcccl}{
\displaystyle\lim_{n\rightarrow \infty}\overline{S_n} & = & \displaystyle\lim_{n\rightarrow \infty}\sum_{i=1}^n (x_i-x_{i-1})\overline{f_i} \\
% 
\\[-1em] 
%
& = & \displaystyle\lim_{n\rightarrow \infty}\sum_{i=1}^n (x_i-x_{i-1})\underline{f_i} & = & \displaystyle\lim_{n\rightarrow \infty}\underline{S_n} \\
%
\\[-1.5em] 
% 
&&&=& \displaystyle \int_a^bf(x)\dx.
}\nn
\end{align}

Although easier than checking \emph{every possible} Riemann sum, checking that the two upper and lower sums converge to a common limit is still a somewhat tedious procedure for checking integrability. In fact it turns out that a sufficient condition on $f$ is that it is continous and bounded on $[a,b]$ \cite{davis}\comment{p7}. The latter condition is not neccessary on a finite interval since all continous functions on a closed finite domain are bounded according to the extreme value theorem. However, if we extend the limits of integration to an infinite interval, for example $[0,\infty)$, then the boundedness of $f$ is not guaranteed by the continuity we need to explicitly demand $|f(x)|<\infty$ for all $x\in[a,b]$.

It is easy to see that the converse is \emph{not} true. Any Riemann integrable function is not automatically continous. Take for example the integral over $[0,1]$ with the step function 
\begin{align}
f(x) = \left\{\mat{lcr}{1 & \text{if} & x>1/2 \\ 0 & \text{else} }\right..
\end{align}
Even though the upper and lower Riemann sums both attain the value $1/2$ in the limit $n\rightarrow \infty$ and the function is Riemann integrable, it demonstrably is not continous. A more careful analysis shows that a less strict but sufficient condition on $f$ is that it be continous \emph{almost everywhere} on $[a,b]$ (i.e. continous on all of the interval, except possibly on a subset $C\subset[a,b]$ with measure zero) \cite{mcdonald}\comment{p72}. With this condition, the converse also holds.

\subsubsection{Newton-Cotes quadrature}
Since the Riemann integral is defined in terms of the limit of a sum, numerical approximations to it arise naturally from any scheme for choosing $\xi_i$ and the partitioning. One of the simplest possible approximations is to take the midpoint value of each sub-interval to be $\xi_i$ with a uniform mesh of equispaced $x_i$s. This constitutes the {\bf midpoint rule} \cite{davis}\comment{p52}, 
\begin{align}
I\approx \sum_{i=1}^n f(x_{i-1}+\Delta x/2)(x_{i}-x_{i-1})=\Delta x\sum_{i=1}^n f(x_{i-1}+\Delta x/2),
\end{align}
where $\Delta x\equiv (x_i-x_{i-1})$ which is the same for all $i$.

Instead of the midpoint, we can use the \emph{average} of the left and right endpoints of the subinterval as $f_i$. Geometrically, this means we are approximating the integral of each sub-interval by the integral over a right trapezoid with base points at $(x_{i-1},0)$ and $(x_i,0)$ and upper point at the function values $(x_{i-1},f(x_{i-1}))$ and $(x_{i},f(x_{i}))$. The resulting approximation is known as the {\bf trapezoidal rule} \cite{hjorthjensen}\comment{p113},
\begin{align}
I\approx \sum_{i=1}^n \frac{f(x_{i-1})+f(x_{i})}{2}(x_i-x_{i-1}) = \Delta x \sum_{i=1}^n \frac{f(x_{i-1})+f(x_{i})}{2}.
\end{align}

Yet another numerical scheme arises from replacing the integrand in each sub-interval with an interpolating polynomial of degree two, which by construction coincides with $f$ at the endpoints and the midpoint. This constitutes {\bf Simpson's rule} \cite{davis}\comment{p57},
\begin{align}
I&\approx \sum_{i=1}^n (x_i-x_{i-1})\left(\frac{f(x_{i-1}}{6}+\frac{4f(x_{i-1}+\Delta x/2)}{6}+\frac{f(x_{i}}{6}\right) \nn\\
&= \frac{\Delta x}{6}\sum_{i=1}^n \Big( f(x_{i-1}) + 4f(x_{i-1}+\Delta x/2) + f(x_i) \Big).
\end{align}

All three approximations are examples of Newton-Cotes quadrature rules, which approximate the intergral by replacing the integrand by interpolating polynomials of order $k$ on each of the $n$ sub-intervals. We can build arbitrarily high order methods by constructing higher order interpolating polynomials within each interval. The interpolation procedure is described for example in \cite{morken}. We have just seen Newton-Cotes method for orders zero (midpoint rule, zero order polynomial [constant]), one (trapezoidal rule, linear polynomial), and two (Simpson's rule, quadratic polynomial). The next few commonly used methods are the third order \emph{Simpson's 3/8 rule} and the fourth order \emph{Boole's rule}.

\subsubsection{Gaussian quadrature}
Note that so far we have assumed the sub-intervals to all be the same size. If we drop this requirement, we can construct more advanced rules which exploit some convenient properties of orthogonal polynomials. Gaussian quadrature rules are a set of schema for numerical intergration in which we extract a \emph{weight function} from the integrand
\begin{align}
\int_a^bf(x)\dx = \int_a^b W(x)g(x)\dx \approx \sum_{i=1}^n w_ig(x_i).
\end{align}
The weight function is associated with a set of orthogonal polynomials, and the integration points $x_i$ are chosen as the zeros of the polynomial of degree $n-1$. Note carefully that $w_i\not=W(x_i)$. The weights $w_i$ can in general be expressed as \cite{krylov}
\begin{align}
w_i=\left(\frac{a_n}{a_{n-1}}\right)\frac{\int_a^b W(x) p_{n-1}(x)^2\dx}{p'_n(x_i)p_{n-1}(x_i)}
\end{align} 
where $p_n(x)$ is the orthogonal polynomial of degree $n$ and $a_n$ is the coefficient of the $x^n$ term in $p_n(x)$. In some cases, the weight function is present in the original integral and the extraction constitutes a strict simplification of the function. For example, with 
Chebyshev polynomials\footnote{The Chebyshev polynomials are solutions to the differential equation 
\begin{align}
(1-x^2)\pder{^2y(x)}{x^2}-x\pder{y(x)}{x} + n^2y(x)=0,
\end{align}
with $n$ a non-negative integer. In general, the solution can be written as \cite{rottmann}\comment{p95} \begin{align}
T_n(x)=\sum_{k=0}^{\lfloor 1/2 \rfloor}{n \choose 2k}(x^2-1)x^{n-2k}.
\end{align}} the weight function takes the form $W(x)=1/\sqrt{1-x^2}$, so trying to apply Gauss-Chebyshev quadrature to the integrand $(x^{10}+x+2)/\sqrt{1-x^2}$ would yield simply $g(x)=x^{10}+x+2$ and we would just have to evaluate $g_i$ according to the zeroes of the $n$th Chebyshev polynomial. Each class of polynomials is associated with a specific interval of integration. For Chebyshev, this is $[-1,1]$. So using our previous example, we note that with only $3!$ integration points (exclamation point for emphasis \emph{and} factorial function) we integrate \emph{exactly}
\begin{align}
I\equiv \int_{-1}^1 \underbrace{\frac{x^{10}+x+2}{\sqrt{1-x^2}}}_{\equiv f(x)}\dx = \sum_{i=1}^6w_i \underbrace{(x^{10}+x+2)}_{g(x)} = \frac{575\pi}{256}.
\end{align}

In general, if $g(x)$ is a polynomial of degree $2n-1$ for a weight function associated with some class of orthogonal polynomials, then the gaussian quadrature rule associated with the same class of polynomials will integrate the original $f(x)$ (recall that $g(x)=f(x)/W(x)$) \emph{exactly} with only $n$ integration points \cite{hjorthjensen}\comment{p119}. 

\subsubsection{Multiple integrals}
Both of the aforementioned  rules are straight forward to extend to higher dimensional integrals. For the Newton-Cotes rules, we can simply apply the rule again to the sum resulting from the application of the rule, i.e.
\begin{align}
I_{\text{2D}} &= \int_a^b \int_a^b f(x,y)\,\mathrm{d}x\,\mathrm{d}y \approx \int_a^b\sum_{i=1}^n \Delta x f(\xi_i,y) \,\mathrm{d}y \nn\\
&\approx \sum_{i=1}^n\sum_{j=1}^n \Delta x\Delta y f(\xi_i,\zeta_j).
\end{align}
Since function evaluations on the endpoints of sub-intervals (sub-areas to be precise) coincide with the endpoints of the neighbouring sub-intervals, a number of points may be evaluated multiple times and thus have a higher \emph{weight} in the final sum. For example, the 1D trapezoidal rule carries weights 
\begin{align}
\mat{ccccccccc}{\nicefrac{1}{2} & 1 & 1 & 1 & \dots & 1 & 1 & 1 & \nicefrac{1}{2}},
\end{align}
since 
\begin{align}
\frac{\Delta x}{2}\sum_{i=1}^n\Big(f(x_{i-1}+f(x_i)\Big) &= \frac{\Delta x}{2}\left[f(x_0) + 2\sum_{i=1}^{n-1}\Big(f(x_i)+f(x_{i+1})\Big)\right] \nn\\
&= \frac{\Delta x}{2}\left[f(x_0) + 2\left(\sum_{i=1}^{n-1}f(x_i)\right)+f(x_{n})\right] \nn\\
&= \frac{\Delta x}{2}\left[f(x_0) + 2f(x_1) + 2f(x_2) + \dots + 2f(x_{n-2}) +2f(x_{n-1}) + f(x_n) \right].
\end{align}
In a similar way, the 2D trapezoidal rule has the weights
\newcommand{\nfh}{\nicefrac{1}{2}}
\begin{align}
\mat{ccccccccccc}{
  \nfh   & 1 & 1 & 1 & \dots & 1 & 1 & 1 & \nfh \\
  1      & 2 & 2 & 2 & \dots & 2 & 2 & 2 & 1 \\
  1      & 2 & 2 & 2 & \dots & 2 & 2 & 2 & 1 \\
  \vdots & \vdots &  \vdots & \vdots & \ddots &\vdots & \vdots&\vdots & \vdots \\
  1      & 2 & 2 & 2 & \dots & 2 & 2 & 2 & 1 \\
  1      & 2 & 2 & 2 & \dots & 2 & 2 & 2 & 1 \\
  \nfh   & 1 & 1 & 1 & \dots & 1 & 1 & 1 & \nfh & ,
}
\end{align}
while the 2D Simpson's rule attains the weights (apart from a factor $\nicefrac{1}{6}$)
\begin{align}
\mat{ccccccccccccc}{
  1      & 4      & 2       & 4      & 2     & \dots  & 2     & 4      & 2      & 4    & 1    \\
  4      & 16     & 8       & 16     & 8     & \dots  & 8     & 16     & 8      & 16   & 4    \\
  2      & 8      & 4       & 8      & 4     & \dots  & 4     & 8      & 4      & 8    & 2    \\
  4      & 16     & 8       & 16     & 8     & \dots  & 8     & 16     & 8      & 16   & 4    \\
  \vdots & \vdots &  \vdots & \vdots & \vdots& \ddots &\vdots & \vdots & \vdots &\vdots& \vdots \\
  4      & 16     & 8       & 16     & 8     & \dots  & 8     & 16     & 8      & 16   & 4    \\
  2      & 8      & 4       & 8      & 4     & \dots  & 4     & 8      & 4      & 8    & 2    \\
  4      & 16     & 8       & 16     & 8     & \dots  & 8     & 16     & 8      & 16   & 4    \\
  1      & 1      & 2       & 4      & 2     & \dots  & 2     & 4      & 2      & 4    & 1 &  .
}
\end{align}

A similar scheme yields multi-dimensional Gaussian quadrature rules, where the total weights become products of the 1D weights.

\end{document}

% \begin{figure}[p!]
% \centering
% \includegraphics[width=12cm]{<fig>.pdf}
% \caption{\label{fig:1}}
% \end{figure}
 
% \lstinputlisting[firstline=1,lastline=2, float=p!, caption={}, label=lst:1]{<code>.m}

